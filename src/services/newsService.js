import { supabase } from "@/lib/supabase";

// Mock LLM response for now since we don't have a backend
const mockLLMResponse = {
    summary: "This is a simulated summary of the news topic. In a real implementation, this would be generated by an AI model analyzing multiple articles.",
    key_points: [
        "Key point 1 about the situation",
        "Key point 2 regarding the impact",
        "Key point 3 on future outlook"
    ],
    left_emphasis: ["Focus on social impact"],
    right_emphasis: ["Focus on economic cost"],
    common_ground: ["Everyone agrees it is important"],
    tags: ["Politics", "Canada"]
};

export const newsService = {
    async getTopics() {
        const { data, error } = await supabase
            .from('news_topics')
            .select('*')
            .order('created_at', { ascending: false });

        if (error) throw error;
        return data;
    },

    async fetchAndAnalyzeNews() {
        // In a real app, this would call a Supabase Edge Function
        // because scraping and LLM calls should happen server-side.
        console.log("Fetching news from sources...");

        // Simulation of the process
        const sources = [
            { name: "CBC", bias: "Left" },
            { name: "Globe and Mail", bias: "Right" }
        ];

        // For now, we'll just return success or mock creating a topic
        // to show it "working" without the actual heavy backend logic.

        // Check if we have recent topics
        const { data: existing } = await supabase
            .from('news_topics')
            .select('*')
            .limit(1);

        if (!existing || existing.length === 0) {
            // Create a dummy topic if none exist
            await supabase.from('news_topics').insert({
                topic: "Sample Political Topic",
                headline: "Parliament Discusses New Bill",
                ai_summary: mockLLMResponse.summary,
                thumbnail_url: "https://images.unsplash.com/photo-1529101091760-6149d4c8df8c?auto=format&fit=crop&w=800&q=80",
                published_date: new Date().toISOString(),
                source_count_left: 2,
                source_count_centre: 1,
                source_count_right: 2,
                left_emphasis: mockLLMResponse.left_emphasis,
                right_emphasis: mockLLMResponse.right_emphasis,
                common_ground: mockLLMResponse.common_ground,
                key_points: mockLLMResponse.key_points,
                tags: mockLLMResponse.tags,
                is_featured: true
            });
        }

        return { success: true };
    }
};
