import { supabase } from "@/lib/supabase";

// Mock LLM response for now since we don't have a backend
const mockLLMResponse = {
    summary: "This is a simulated summary of the news topic. In a real implementation, this would be generated by an AI model analyzing multiple articles.",
    key_points: [
        "Key point 1 about the situation",
        "Key point 2 regarding the impact",
        "Key point 3 on future outlook"
    ],
    left_emphasis: ["Focus on social impact"],
    right_emphasis: ["Focus on economic cost"],
    common_ground: ["Everyone agrees it is important"],
    tags: ["Politics", "Canada"]
};

export const newsService = {
    async getTopics() {
        const { data, error } = await supabase
            .from('news_topics')
            .select('*')
            .order('created_at', { ascending: false });

        if (error) throw error;
        return data;
    },

    async fetchAndAnalyzeNews() {
        console.log("Invoking fetch-rss-news Edge Function...");

        const { data, error } = await supabase.functions.invoke('fetch-rss-news');

        if (error) {
            console.error("Error invoking function:", error);
            throw error;
        }

        console.log("Edge function response:", data);
        return data;
    }
};
